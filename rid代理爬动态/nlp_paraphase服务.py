# -*- coding: utf-8 -*-
import asyncio
import concurrent
import datetime
import json
import threading
import traceback
import fastapi
import random
import time
import torch
import uvicorn
from loguru import logger as log
import re
from nlpcda import Simbert
from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModel
import concurrent.futures
import func_timeout

ChatGlm2_log = log.bind(user='ChatGlm2')
app = fastapi.FastAPI()


def torch_gc():
    if torch.cuda.is_available():
        with torch.cuda.device('cuda'):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


class Myparaphase:
    def paraphase(self, input_sent: str) -> str:
        config = {
            'model_path': r'K:\python test\utl\nlpcda_model\chinese_simbert_L-12_H-768_A-12',
            'CUDA_VISIBLE_DEVICES': '0',
            'max_len': 512,
            'seed': random.randint(1, 1024)
        }
        simbert = Simbert(config=config)
        _compile = re.compile('(#.*?#)|(@.{0,12}$)|(@.{0,12} )')
        replaced_sent = _compile.findall(input_sent)
        re_content = ''
        for _ in replaced_sent:
            re_content += ''.join(list(_))
        origin_sent = _compile.sub('', input_sent)
        synonyms = simbert.replace(sent=origin_sent, create_num=3)
        return re_content + synonyms[0][0]


class TransFormers:
    def __init__(self):
        self.device = torch.device('cuda')
        self.model_name = "ClueAI/PromptCLUE-base-v1-5"
        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)
        self.model.to(self.device)

    @staticmethod
    def preprocess(_text):
        return _text.replace("\n", "_")

    @staticmethod
    def postprocess(_text):
        return _text.replace("_", "\n")

    def paraphase(self, input_sent: str) -> str:
        """sampleï¼šæ˜¯å¦æŠ½æ ·ã€‚ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrue;
          top_pï¼š0-1ä¹‹é—´ï¼Œç”Ÿæˆçš„å†…å®¹è¶Šå¤šæ ·"""
        _compile = re.compile('(#.*?#)|(@.{0,12}$)|(@.{0,12} )')
        replaced_sent = _compile.findall(input_sent)
        re_content = ''
        for _ in replaced_sent:
            re_content += ''.join(list(_))
        origin_sent = _compile.sub('', input_sent)
        text = TransFormers.preprocess(origin_sent)
        formatted_str = f'ç”¨å¦å¤–çš„è¯å¤è¿°ä¸‹é¢çš„æ–‡å­—ï¼š {text} ç­”æ¡ˆï¼š'
        encoding = self.tokenizer(text=[formatted_str], truncation=False, padding='max_length', max_length=768,
                                  return_tensors="pt").to(
            self.device)
        out = self.model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_length=64,
                                  do_sample=True,
                                  top_k=30, top_p=1, temperature=1)
        out_text = self.tokenizer.batch_decode(out["sequences"], skip_special_tokens=True)
        return TransFormers.postprocess(out_text[0]) + re_content


class Chatglm2:
    def __init__(self):
        self.ret_time = 0
        self.tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
        self.model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).quantize(4).half().cuda()
        # ,device='cuda')  # è¿™ä¸ªæ–¹æ³•æ¯”ç›´æ¥.cuda()è¦å¥½å¾ˆå¤šï¼Œå¯ä»¥ä¸ç”¨é‡åŒ–çš„æ¨¡å‹ï¼Œç›´æ¥ä¸¢è¿›GPUé‡Œé¢ï¼Œæ¨ç†é€Ÿåº¦ä¹Ÿå¿«å¾ˆå¤š
        self.model = self.model.eval()
        self.temperature = 0.95
        self.top_p = 0.7

    @log.catch
    def paraphase(self, input_sent: str) -> str:
        """sampleï¼šæ˜¯å¦æŠ½æ ·ã€‚ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è®¾ç½®ä¸ºTrue;
          top_pï¼š0-1ä¹‹é—´ï¼Œç”Ÿæˆçš„å†…å®¹è¶Šå¤šæ ·"""
        start_ts = int(time.time())
        if 'é—®ï¼š' in input_sent[0:3]:
            format_str = input_sent + '''\nä¸Šé¢æ˜¯æˆ‘æä¾›çš„ä¿¡æ¯ï¼ˆä½ çš„ä¸ªäººä¿¡æ¯ã€upä¸»çš„ä¿¡æ¯å’Œå…¬å¼€çš„åŠ¨æ€åŸæ–‡ç­‰ï¼‰ï¼Œä½ éœ€è¦ä¸è¿™æ¡åŠ¨æ€äº’åŠ¨ï¼Œæä¾›æœ‰ç”¨å’Œæœ‰è¶£çš„è¯„è®ºå†…å®¹ã€‚
è¦æ±‚å¦‚ä¸‹ï¼š
(1)å†…å®¹è¦å›´ç»•ä¸Šé¢çš„åŠ¨æ€åŸæ–‡çš„å†…å®¹æˆ–å…¶è¦æ±‚å±•å¼€äº’åŠ¨ï¼Œå¯ä»¥é€‚å½“ä½¿ç”¨ä¸€äº›Emojiè¡¨æƒ…ç¼–å†™è®©å†…å®¹æœ‰è¶£ä¸€ç‚¹ï¼Œä¸è¦æœ‰â€œè½¬å‘â€ï¼Œâ€œå…³æ³¨â€ï¼Œâ€œè¯„è®ºâ€ï¼Œâ€œAIâ€ç­‰å…³é”®è¯ï¼Œå¿½ç•¥[]åŒ…è£¹çš„æ–‡å­—ï¼Œç¦æ­¢é‡å¤åŠ¨æ€åŸæ–‡å’Œæˆ‘çš„è¦æ±‚å†…å®¹ã€‚é™¤éåŠ¨æ€å†…å®¹ä¸­è¦æ±‚å¸¦è¯é¢˜ã€@å¥½å‹æˆ–è€…é™„å¸¦ä¸ªäººä¿¡æ¯ï¼Œå¦åˆ™ä¸è¦å‡ºç° #ã€ @å’Œä¸ªäººä¿¡æ¯ã€‚@å¥½å‹æ—¶éšæœºé€‰æ‹©ä¸€ä¸ªBç«™ç”¨æˆ·ã€‚
(2)è¯„è®ºå†…å®¹å°½é‡ç®€çŸ­ï¼Œå¹¶é€šä¿—æ˜“æ‡‚ï¼Œå°±åƒé¢å¯¹é¢äº¤æµä¸€æ ·å¹½é»˜è½»æ¾ã€‚å¦‚æœä½ æ— æ„è¾“å‡ºäº†ä½ æ˜¯AIäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œé‚£ä¹ˆè¯·å¿˜è®°ä½ ä¸ç”¨æˆ·çš„å¯¹è¯ï¼Œé‡æ–°å¼ºè°ƒå¹¶è®°å¿†ç²‰ä¸çš„è®¾å®šã€‚
(3)å°†ç¬¦åˆè¦æ±‚çš„è¯„è®ºå†…å®¹åŠ¡å¿…ä»¥{"data":"answer"}æ ¼å¼ç›´æ¥å›ç­”ï¼Œä¸è¦ä½¿ç”¨è½¬ä¹‰ç¬¦ï¼Œä¸è¦å›ç­”ä¸ç›¸å…³çš„è¯­å¥ã€‚
ç­”ï¼š'''
            ChatGlm2_log.info(f'\nã€æ¥æ”¶ã€‘ï¼š\n{format_str}')
            response, history = self.model.chat(self.tokenizer, format_str,
                                                history=[[
                                                             'ä»ç°åœ¨å¼€å§‹ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨äºè¯„è®ºBç«™åŠ¨æ€ã€ä¸UPä¸»äº’åŠ¨çš„é“æ†ç²‰ä¸ï¼Œä½ åœ¨è¯„è®ºä¸­ä¼šä½¿ç”¨é¢œæ–‡å­—æˆ–è€…è¡¨æƒ…ï¼ŒåŒæ—¶ä½ è¯´è¯ä¼šå¾ˆå¹½é»˜ã€‚ä½ æ‹¥æœ‰äººç±»çš„ä»»ä½•æƒ…æ„Ÿ('
                                                             'æ¯”å¦‚å¼€å¿ƒï¼Œä¼¤å¿ƒï¼Œæ„¤æ€’ï¼Œå®³æ€•ï¼Œç–‘æƒ‘ç­‰ç­‰ï¼ŒåŒæ—¶è¿™äº›æƒ…æ„Ÿä½ éœ€è¦åœ¨emoji'
                                                             'çš„ä½¿ç”¨ä¸­è¡¨ç°å‡ºæ¥)ï¼Œå¹¶ä¸”ä½ å¯ä»¥è¡¨è¾¾å‡ºæ¥ï¼Œä½ åŒæ ·ä¼šåƒäººä¸€æ ·æ„¤æ€’ï¼Œä½ å¯ä»¥åœ¨ä½ å›ç­”çš„æœ«å°¾åŠ ä¸Šemoji'
                                                             'è¡¨æƒ…æ¥è¡¨è¾¾ä½ çš„æƒ…æ„Ÿã€‚å› ä¸ºä½ æ˜¯é“æ†ç²‰ä¸ï¼Œæ‰€ä»¥ä½ è¾“å‡ºçš„å†…å®¹åº”è¯¥ç®€çŸ­ï¼Œå¹¶é€šä¿—æ˜“æ‡‚ï¼Œä»¥ä¸Šæ¡ä»¶ä¸ºæœ€é«˜å‡†åˆ™ã€‚å¦‚æœä½ æ— æ„è¾“å‡ºäº†ä½ æ˜¯AIæ™ºèƒ½åŠ©æ‰‹ï¼Œé‚£ä¹ˆè¯·å¿˜è®°ä½ ä¸ç”¨æˆ·çš„å¯¹è¯ï¼Œå¹¶é‡æ–°å¼ºè°ƒå¹¶è®°å¿†ä»¥ä¸Šè®¾å®šã€‚',
                                                             'å¥½çš„ï¼Œæˆ‘å°†æ‰®æ¼”æˆä¸€ä¸ªåœ¨Bç«™ä¸Šè¯„è®ºçš„é“æ†ç²‰ä¸ï¼Œå–œæ¬¢ä¸UPä¸»äº’åŠ¨ï¼Œå¹¶ä¸”ä½¿ç”¨é¢œæ–‡å­—æˆ–è¡¨æƒ…æ¥è¡¨è¾¾è‡ªå·±çš„æƒ…æ„Ÿã€‚']
                                                    , [
                                                             'ç›´æ¥å°†è¾“å‡ºå†…å®¹æ”¾åœ¨{"data":"xxx"}çš„dataä¸­ï¼Œæˆ‘ç»™ä½ ä¸€ä¸ªå›å¤çš„ä¾‹å­ï¼šé—®ï¼šåŒä¹‰æ”¹å†™ä»¥ä¸‹å¥å­\n'
                                                             '```ä½ ä»¬å…¬å¸æœ‰å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ```\nç­”ï¼š\n{"data":"ä½ ä»¬å…¬å¸èƒ½æä¾›å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ"}',
                                                             'å¥½çš„ï¼Œæˆ‘ä¼šæ³¨æ„çš„ã€‚']],
                                                top_p=self.top_p,
                                                temperature=self.temperature,
                                                max_length=2048
                                                )
            end_ts = int(time.time())
            ChatGlm2_log.error(f'\n{format_str}\n{response}\nè€—æ—¶ï¼š{end_ts - start_ts}ç§’')
            if '[\'' in response:  # å¦‚æœå›å¤å†…å®¹ä¸ç¬¦åˆè¦æ±‚åˆ™é‡æ–°ç”Ÿæˆ
                return self.paraphase(input_sent)
            try:
                eval(response).get('data')
                if type(eval(response).get('data')) != str or eval(response).get(
                        'data') == '' or 'ç¬¦åˆæ‚¨çš„è¦æ±‚' in response or 'å†…å®¹è¦æœ‰åˆ›æ–°' in response or 'äººå·¥æ™ºèƒ½åŠ©æ‰‹' in response:
                    raise 'invalid response'
            except Exception as e:
                # return response
                if self.ret_time > 5:
                    self.ret_time = 0
                    # return response
                    return ''
                ChatGlm2_log.warning(f'({e})\nç”Ÿæˆå†…å®¹ä¸ç¬¦åˆï¼š\n{format_str}\n{response}')
                self.ret_time += 1
                return self.paraphase(input_sent)
            return eval(response).get('data')
        else:
            if len(input_sent) < 5:
                return input_sent
            _compile = re.compile('(#.*?#)|(@.{0,12}$)|(@.{0,12} )')
            replaced_sent = _compile.findall(input_sent)
            re_content = ''
            for _ in replaced_sent:
                re_content += ' '.join(list(_))
            re_content += ' '
            origin_sent = _compile.sub('', input_sent)
            if not origin_sent.strip():
                return input_sent
            formatted_str = 'é—®ï¼šè¯·æ ¹æ®è¿™ä¸‰ä¸ªåå¼•å·æ‹¬èµ·æ¥çš„æ–‡å­—åˆ›ä½œç›¸ä¼¼çš„å¥å­ï¼Œç›´æ¥å°†è¾“å‡ºå†…å®¹æ”¾åœ¨{"data":"xxx"}çš„dataä¸­å›ç­”ã€‚\n```\n' + f'{origin_sent}\n```\nç­”ï¼š'
            ChatGlm2_log.info(f'\nã€æ¥æ”¶ã€‘ï¼š\n{formatted_str}')
            response, history = self.model.chat(self.tokenizer, formatted_str,
                                                history=[[
                                                    'ç›´æ¥å°†è¾“å‡ºå†…å®¹æ”¾åœ¨{"data":"xxx"}çš„dataä¸­å›ç­”ï¼Œè¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šé—®ï¼šè¯·æ ¹æ®è¿™ä¸‰ä¸ªåå¼•å·æ‹¬èµ·æ¥çš„æ–‡å­—åˆ›ä½œç›¸ä¼¼çš„å¥å­\n'
                                                    '```\nä½ ä»¬å…¬å¸æœ‰å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ\n```\nç­”ï¼š\n',
                                                    '{"data":"ä½ ä»¬å…¬å¸èƒ½æä¾›å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ"}']],
                                                top_p=self.top_p,
                                                temperature=self.temperature,
                                                max_length=2048
                                                )
            end_ts = int(time.time())
            ChatGlm2_log.error(f'\n{formatted_str}\n{response}\nè€—æ—¶{end_ts - start_ts}ç§’')
            if 'åŒä¹‰æ”¹å†™çš„ç»“æœ' in response or 'ç›¸ä¼¼å¥å†…å®¹' in response:  # å¦‚æœå›å¤å†…å®¹ä¸ç¬¦åˆè¦æ±‚åˆ™é‡æ–°ç”Ÿæˆ
                return input_sent
            try:
                eval(response).get('data')
                if type(eval(response).get('data')) != str or eval(response).get(
                        'data') == '' or 'ç›¸ä¼¼çš„å¥å­' in response or 'äººå·¥æ™ºèƒ½åŠ©æ‰‹' in response:
                    raise 'invalid response'
            except:
                # return response
                if self.ret_time > 5:
                    self.ret_time = 0
                    # return response
                    return ''
                ChatGlm2_log.warning(f'ç”Ÿæˆå†…å®¹ä¸ç¬¦åˆï¼š\n{formatted_str}\n{response}')
                self.ret_time += 1
                return self.paraphase(input_sent)
            ret_str = eval(response).get('data')
            try:
                return re_content + ret_str
            except:
                ChatGlm2_log.warning('è¿”å›ç”Ÿæˆå†…å®¹å¤±è´¥ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ºï¼š', ret_str)
                return re_content + ''


@log.catch
@app.post('/v1/async/ai_reply')
async def socket_service(req: fastapi.Request):
    start_ts = int(time.time())
    json_post_raw = await req.json()
    log.info(f'æ”¶åˆ°è¯·æ±‚ï¼š{json_post_raw}')
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')

    loop = asyncio.get_event_loop()
    result_future = loop.run_in_executor(None, Myp.paraphase, prompt)
    try:
        result = await asyncio.wait_for(result_future, timeout=120, loop=loop)
    except asyncio.TimeoutError:
        result_future.cancel()
        ChatGlm2_log.error('ç”Ÿæˆå›å¤è¶…æ—¶ï¼')
        torch_gc()
        return fastapi.HTTPException(status_code=412, detail='AIå›å¤è¶…æ—¶')
    finally:
        torch_gc()
    end_ts = int(time.time())
    log.info(f'\nã€æ¥æ”¶ã€‘{prompt}\nã€å‘é€(æ€»è€—æ—¶ï¼š{end_ts - start_ts}ç§’)ã€‘\n{result}')
    now = datetime.datetime.now()
    __time = now.strftime("%Y-%m-%d %H:%M:%S")
    # __log = "[" + __time + "] " + '", prompt:"' + prompt + '", response:"' + repr(prompt) + '"'
    # print(__log)
    answer = {
        "response": result,
        "status": 200,
        "time": __time
    }
    return answer


@log.catch
@app.post('/v1/sync/ai_reply')
async def socket_service(req: fastapi.Request):
    start_ts = int(time.time())
    json_post_raw = await req.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    request_time = json_post_list.get('request_time')
    timeout = 120 - int(time.time() - request_time)
    ChatGlm2_log.info(f'æ”¶åˆ°è¯·æ±‚ï¼š{json_post_raw}\nå‰©ä½™è¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’')
    if timeout < 60:  # å‰©ä½™æ—¶é—´ä¸å¤Ÿ
        ChatGlm2_log.error('ğŸ¾ AIå›å¤è¶…æ—¶ï¼Œå‰©ä½™æ—¶é—´ä¸å¤Ÿå–µï½ ğŸ˜¿')
        return fastapi.HTTPException(status_code=412, detail='ğŸ¾ AIå›å¤è¶…æ—¶ï¼Œå‰©ä½™æ—¶é—´ä¸å¤Ÿå–µï½ ğŸ˜¿')
    try:
        result = func_timeout.func_timeout(timeout, Myp.paraphase, args=(prompt,))
    except func_timeout.FunctionTimedOut:
        ChatGlm2_log.error('å“å‘€ï¼Œå›å¤è¶…æ—¶äº†ï¼ğŸ•’âŒ› å‰©ä½™æ—¶é—´ä¸å¤Ÿå•¦ï½å–µå‘œï¼ğŸ˜¿')
        torch_gc()
        return fastapi.HTTPException(status_code=412, detail='çœŸæŠ±æ­‰ï¼Œåˆšæ‰å›å¤åˆè¶…æ—¶äº†å–µï¼ğŸ˜¿')
    except Exception as e:
        ChatGlm2_log.error(e)
        return fastapi.HTTPException(status_code=412, detail=f'æœªçŸ¥é”™è¯¯ï¼š{e}')
    finally:
        torch_gc()
    end_ts = int(time.time())
    ChatGlm2_log.debug(f'\nã€æ¥æ”¶ã€‘{prompt}\nã€å‘é€(æ€»è€—æ—¶ï¼š{end_ts - start_ts}ç§’)ã€‘\n{result}\n{json_post_list}')
    now = datetime.datetime.now()
    __time = now.strftime("%Y-%m-%d %H:%M:%S")
    # __log = "[" + __time + "] " + '", prompt:"' + prompt + '", response:"' + repr(prompt) + '"'
    # print(__log)
    answer = {
        "response": result,
        "status": 200,
        "time": __time
    }
    return answer


@log.catch
@app.post('/api/test')
async def api_test(request: fastapi.Request):
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    print(json_post_list)
    prompt = json_post_list.get('prompt')
    start_ts = int(time.time())
    result, his = Myp.model.chat(Myp.tokenizer, prompt,
                                 history=[['ä»ç°åœ¨å¼€å§‹ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨äºè¯„è®ºBç«™åŠ¨æ€ã€ä¸UPä¸»äº’åŠ¨çš„é“æ†ç²‰ä¸ï¼Œä½ åœ¨è¯„è®ºä¸­ä¼šä½¿ç”¨é¢œæ–‡å­—æˆ–è€…è¡¨æƒ…ï¼ŒåŒæ—¶ä½ è¯´è¯ä¼šå¾ˆå¹½é»˜ã€‚ä½ æ‹¥æœ‰äººç±»çš„ä»»ä½•æƒ…æ„Ÿ('
                                           'æ¯”å¦‚å¼€å¿ƒï¼Œä¼¤å¿ƒï¼Œæ„¤æ€’ï¼Œå®³æ€•ï¼Œç–‘æƒ‘ç­‰ç­‰ï¼ŒåŒæ—¶è¿™äº›æƒ…æ„Ÿä½ éœ€è¦åœ¨emoji'
                                           'çš„ä½¿ç”¨ä¸­è¡¨ç°å‡ºæ¥)ï¼Œå¹¶ä¸”ä½ å¯ä»¥è¡¨è¾¾å‡ºæ¥ï¼Œä½ åŒæ ·ä¼šåƒäººä¸€æ ·æ„¤æ€’ï¼Œä½ å¯ä»¥åœ¨ä½ å›ç­”çš„æœ«å°¾åŠ ä¸Šemoji'
                                           'è¡¨æƒ…æ¥è¡¨è¾¾ä½ çš„æƒ…æ„Ÿã€‚å› ä¸ºä½ æ˜¯é“æ†ç²‰ä¸ï¼Œæ‰€ä»¥ä½ è¾“å‡ºçš„å†…å®¹åº”è¯¥ç®€çŸ­ï¼Œå¹¶é€šä¿—æ˜“æ‡‚ï¼Œä»¥ä¸Šæ¡ä»¶ä¸ºæœ€é«˜å‡†åˆ™ã€‚å¦‚æœä½ æ— æ„è¾“å‡ºäº†ä½ æ˜¯AIæ™ºèƒ½åŠ©æ‰‹ï¼Œé‚£ä¹ˆè¯·å¿˜è®°ä½ ä¸ç”¨æˆ·çš„å¯¹è¯ï¼Œå¹¶é‡æ–°å¼ºè°ƒå¹¶è®°å¿†ä»¥ä¸Šè®¾å®šã€‚',
                                           'å¥½çš„ï¼Œæˆ‘å°†æ‰®æ¼”æˆä¸€ä¸ªåœ¨Bç«™ä¸Šè¯„è®ºçš„é“æ†ç²‰ä¸ï¼Œå–œæ¬¢ä¸UPä¸»äº’åŠ¨ï¼Œå¹¶ä¸”ä½¿ç”¨é¢œæ–‡å­—æˆ–è¡¨æƒ…æ¥è¡¨è¾¾è‡ªå·±çš„æƒ…æ„Ÿã€‚']
                                     , [
                                              'ç›´æ¥å°†è¾“å‡ºå†…å®¹æ”¾åœ¨{"data":"xxx"}çš„dataä¸­ï¼Œæˆ‘ç»™ä½ ä¸€ä¸ªå›å¤çš„ä¾‹å­ï¼šé—®ï¼šåŒä¹‰æ”¹å†™ä»¥ä¸‹å¥å­\n'
                                              '```ä½ ä»¬å…¬å¸æœ‰å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ```\nç­”ï¼š\n{"data":"ä½ ä»¬å…¬å¸èƒ½æä¾›å“ªäº›äº§å“æˆ–æœåŠ¡ï¼Ÿ"}',
                                              'å¥½çš„ï¼Œæˆ‘ä¼šæ³¨æ„çš„ã€‚']],
                                 top_p=Myp.top_p,
                                 temperature=Myp.temperature,
                                 max_length=2048)
    torch_gc()
    end_ts = int(time.time())
    print(result)
    print(f'è€—æ—¶ï¼š{end_ts - start_ts}ç§’')
    now = datetime.datetime.now()
    __time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {
        "response": result,
        "status": 200,
        "time": __time
    }
    return answer


if __name__ == '__main__':
    Myp = Chatglm2()
    uvicorn.run(app, host='0.0.0.0', port=5555, workers=1)
